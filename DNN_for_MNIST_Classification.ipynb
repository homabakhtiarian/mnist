{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the relevent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "#loads the data in a 2-tuple structure [input, target]\n",
    "#provides a tuple containing info about version, features, # samples of the dataset, ...\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "#By default tensorflow datasets have train and test datasets but NO validation dataset, Train = 60000, Test = 10000\n",
    "\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "#we can either count the number of training samples or we can use the mnist_info variable we created earlier.\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    " \n",
    "#we would like to scale our data in some way to make the result more numerically stable. \n",
    "#In this case we will simply prefer to have inputs between 0 and 1.\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#Since we'll be batching we'd better shuffle the data it should be as randomly spread as possible so that batching works as intended.\n",
    "#If our data is not shuffled, This will confuse the stochastic gradient descent algorithm because each batch is homogenous inside it but completely different from all other batches, causing the loss to differ greatly.\n",
    "#This buffer size parameter is used in cases when we are dealing with enormous datasets.\n",
    "#In such cases we can't shuffle the whole data set in one go because we can't possibly fit it all in the memory of the computer.\n",
    "#So instead we must instruct tensorflow to take samples ten thousand at a time shuffle them and then take the next ten thousand.       \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "#we will be using mini batch gradient descent to train our model. This is the most efficient way to perform deep learning as the tradeoff between accuracy and speed is optimal.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "#since we won't be back propagating on the validation data but only forward propagating we don't really need to batch.\n",
    "#So whenever we validate or test we simply forward propagate Once. When batching we usually find the average loss and average accuracy; during validation and testing We want the exact values.\n",
    "#However the model expects our validation set in batch form too. In this way we'll create a new column in our tensor indicating that the model should take the whole validation dataset at once when it utilizes it.\n",
    "#To handle our test data We don't need to batch it either. We'll take the same approach we use with the validation set.\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "#our validation data must have the same shape and object properties as the train and test data. The mnist data is itterable and in tuple format As we set the argument as_supervised to true. \n",
    "#Therefore we must extract and convert the validation inputs and targets appropriately.\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "                           ])\n",
    "#When we are creating a classifier the activation function of the output layer must transform the values into probabilities. Therefore we must opt for the softMax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Optimizer and the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 16s - loss: 0.4241 - accuracy: 0.8833 - val_loss: 0.2288 - val_accuracy: 0.9313\n",
      "Epoch 2/10\n",
      "540/540 - 8s - loss: 0.1892 - accuracy: 0.9448 - val_loss: 0.1535 - val_accuracy: 0.9545\n",
      "Epoch 3/10\n",
      "540/540 - 8s - loss: 0.1449 - accuracy: 0.9569 - val_loss: 0.1260 - val_accuracy: 0.9632\n",
      "Epoch 4/10\n",
      "540/540 - 8s - loss: 0.1193 - accuracy: 0.9641 - val_loss: 0.1133 - val_accuracy: 0.9642\n",
      "Epoch 5/10\n",
      "540/540 - 7s - loss: 0.1003 - accuracy: 0.9694 - val_loss: 0.1048 - val_accuracy: 0.9692\n",
      "Epoch 6/10\n",
      "540/540 - 6s - loss: 0.0874 - accuracy: 0.9740 - val_loss: 0.0857 - val_accuracy: 0.9728\n",
      "Epoch 7/10\n",
      "540/540 - 6s - loss: 0.0756 - accuracy: 0.9764 - val_loss: 0.0800 - val_accuracy: 0.9763\n",
      "Epoch 8/10\n",
      "540/540 - 9s - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.0792 - val_accuracy: 0.9742\n",
      "Epoch 9/10\n",
      "540/540 - 6s - loss: 0.0602 - accuracy: 0.9816 - val_loss: 0.0671 - val_accuracy: 0.9790\n",
      "Epoch 10/10\n",
      "540/540 - 10s - loss: 0.0553 - accuracy: 0.9834 - val_loss: 0.0652 - val_accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29f59ec2948>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2, validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 3s 3s/step - loss: 0.0922 - accuracy: 0.9746"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 97.46%.\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%.'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
